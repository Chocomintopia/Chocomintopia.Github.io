### Chapter 2  Multi-armed Bandits

本章主要针对 **nonassociative** 问题，即无需考虑每一步行动之间的影响，以及环境对行动的影响，实际上是一种很理想化的问题，实用性不大，但作为强化学习入门还不错。

强化学习和其它机器学习方法最大的区别，“**evaluates** the actions taken rather than **instructs** by giving correct actions”

- 评价性反馈（Evaluative Feedback）：知道每一步 action 的好坏程度，但不知道这个 action 是否是最好 / 最差
- 指导性反馈（Instructive Feedback）：直接得知最优 action

#### 2.1 A k-armed Bandit Problem

##### Background



参考：[Sutton RL2020](http://incompleteideas.net/sutton/book/RLbook2020.pdf)  [ZHANGWP-RLAI_2](https://www.zhangwp.com/notes/book-reading/RLAI/RLAI_2)